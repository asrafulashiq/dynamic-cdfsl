# pl trainer params
logger: False
checkpoint_callback: True
default_root_dir: null
num_nodes: 1
tpu_cores: null
check_val_every_n_epoch: 5
fast_dev_run: False
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0
accelerator: null
sync_batchnorm: False
num_sanity_val_steps: 0
resume_from_checkpoint: null
benchmark: False
reload_dataloaders_every_epoch: False
terminate_on_nan: False
auto_scale_batch_size: False
automatic_optimization: null
progress_bar_refresh_rate: 0

log_every_n_steps: 100000
replace_sampler_ddp: True
weights_save_path: ckpt/${model_name}${suffix}
max_epochs: 100
gpus: 1

deterministic: true
